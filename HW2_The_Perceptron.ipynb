{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **HW1a The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGVmKzgG2Ium",
        "outputId": "1e36b189-8dfd-4ca6-cd83-5d8806a79c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-07 02:00:59--  http://www.cse.unt.edu/~blanco/csce5218/hw1a/train.dat\n",
            "Resolving www.cse.unt.edu (www.cse.unt.edu)... 129.120.151.91\n",
            "Connecting to www.cse.unt.edu (www.cse.unt.edu)|129.120.151.91|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11244 (11K) [application/x-ns-proxy-autoconfig]\n",
            "Saving to: ‘train.dat’\n",
            "\n",
            "\rtrain.dat             0%[                    ]       0  --.-KB/s               \rtrain.dat           100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-07 02:00:59 (197 MB/s) - ‘train.dat’ saved [11244/11244]\n",
            "\n",
            "--2022-02-07 02:00:59--  http://www.cse.unt.edu/~blanco/csce5218/hw1a/test.dat\n",
            "Resolving www.cse.unt.edu (www.cse.unt.edu)... 129.120.151.91\n",
            "Connecting to www.cse.unt.edu (www.cse.unt.edu)|129.120.151.91|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844 (2.8K) [application/x-ns-proxy-autoconfig]\n",
            "Saving to: ‘test.dat’\n",
            "\n",
            "test.dat            100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-07 02:00:59 (370 MB/s) - ‘test.dat’ saved [2844/2844]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the datasets\n",
        "!wget http://www.cse.unt.edu/~blanco/csce5218/hw1a/train.dat\n",
        "!wget http://www.cse.unt.edu/~blanco/csce5218/hw1a/test.dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A69DxPSc8vNs",
        "outputId": "a34b46e6-940e-42f3-c3fd-4a8351d7ce36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
          ]
        }
      ],
      "source": [
        "# Take a peek at the datasets\n",
        "!head train.dat\n",
        "!head test.dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXHLhnhwiBR"
      },
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(file_name, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = list(map(int, instance.strip().split('\\t')))\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [-1] + instance\n",
        "        data += [instance]\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    #TODO: Return dot product of array 1 and array 2\n",
        "    dot_prod = sum([i * j for i, j in zip(array1, array2)])\n",
        "    return dot_prod\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    #TODO: Return outpout of sigmoid function on x\n",
        "\n",
        "    sigmoid_function = (1 / (1 + math.exp(-x)))\n",
        "    return sigmoid_function\n",
        "\n",
        "# The output of the model, which for the perceptron is \n",
        "# the sigmoid function applied to the dot product of \n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "    #TODO: return the output of the model \n",
        "    return sigmoid(dot_product(weight,instance))\n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):\n",
        "    #TODO: return the prediction of the model\n",
        "    predict_value = output(weights, instance)\n",
        "    if(predict_value>=0.5):\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    # You do not to write code like this, but get used to it\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate) \n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "\n",
        "    #TODO: name this step\n",
        "    # Initialising weights to \"0\" with the length of number of input parameters\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            #TODO: name these steps\n",
        "            # Assigning (Calculating) the values to the neurons value\n",
        "            in_value = dot_product(weights, instance)\n",
        "            output = sigmoid(in_value)\n",
        "            error = instance[-1] - output\n",
        "            #TODO: name these steps\n",
        "            # Updating the weights based on the error value\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBZuMlAwiBT"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "50YvUza-BYQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67982c3c-d79b-4078-86a4-fca260f7208f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)\n",
        "\n",
        "Answer:<br>\n",
        "If the code snippet is changed(i.e., dot_product is removed ), the weights will not be updated, resulting in the model being unable to distinguish between two classes which results in the  poor accuracy. The activation function,gives the  nonlinearity in the data, generates the gradients for updating weights during back propagation, and aids the model in learning more complicated patterns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G-VKJOUu2BTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d26cc9b-5afd-43ee-e4c4-00a786cce738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
            "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
            "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
            "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0\n",
            "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
            "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
            "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
            "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "  for tr_size in tr_percent:\n",
        "    for epochs in num_epochs:\n",
        "      size =  round(len(instances_tr)*tr_size/100)\n",
        "      pre_instances = instances_tr[0:size]\n",
        "      weights = train_perceptron(pre_instances, lr, epochs)\n",
        "      accuracy = get_accuracy(weights, instances_te)\n",
        "      print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO: Add your answer here (code and text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38rA_Kp3wiBX"
      },
      "source": [
        "Answer (3) :<br>\n",
        "**3a Question:**<br>\n",
        "Do you need to train with all the training dataset to get the highest accuracy with the test dataset? <br>\n",
        "**3a Ans:** <br>\n",
        "tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0<br>\n",
        "tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
        "<br>\n",
        "\n",
        "In the above example we can observe that as the training data increased from the 100 to 200 keeping all the hyperparameters constant the accuracy of the model has been increased. i.e., by providing more data to the model helps the extarct more insights from the data.\n",
        "<br>\n",
        "<br>\n",
        "**3b Question**<br>\n",
        "How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?<br>\n",
        "tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "<br>\n",
        "tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "<br>\n",
        "**3b Answer**<br>\n",
        "In the above scenario, although we provide the training data more compared to the previous set, the accuracy not only depends on the amount of data but it also depends on the other hyperparameters as well in the above example the learning rate of tr:100 is 0.05 but for tr:200 it is 0.005 where learning rate indicates how much the model need to correct from the past value. So, here the learning rate for 100 is greater than tr 200 so the model is learning less compared to training data 100 which resulted in the less accuracy.\n",
        "\n",
        "**3c Question**<br>\n",
        "C. Can you get higher accuracy with additional hyperparameters (higher than 80.0)?<br>\n",
        "**3c Answer**<br>\n",
        "It is possible to get higher accuracy with additional hyperparameter depending on the search space we are looking. I we can look at the right search space we can get the higher accuracy with less hyperparameters as well.\n",
        "<br>**3d Question**<br>\n",
        "Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?<br>\n",
        "**3d Answer**<br>\n",
        "Yes,it is worth to train the data with more epoch until the model gets overfit, we can see that increase in the accuracy with the increase in the epochs untill it get overfit. Once the model gets overfit the increasing the epochs will result in the poor accuracy compared to previous epochs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HW2_The_Perceptron.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}